{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd16a7b",
   "metadata": {},
   "source": [
    "<div class=\"title\">Implementing an SVC Model</div>\n",
    "<div class=\"subtitle\">Métodos Avanzados en Aprendizaje Automático</div>\n",
    "<div class=\"author\">Carlos María Alaíz Gudín &mdash; Universidad Autónoma de Madrid</div>\n",
    "\n",
    "---\n",
    "<div style=\"font-size: large; font-weight: bold; margin-left: 6em;\"><p>Team Number and Names: <u>???????</u></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1d8e0",
   "metadata": {},
   "source": [
    "**Configuration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the general configuration of Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<head><link rel=\"stylesheet\" href=\"style.css\"></head>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the packages to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages.\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Custom packages.\n",
    "from ml import (\n",
    "\tgenerate_dataset_moons,\n",
    "\tplot_dataset_clas,\n",
    "\tplot_svc,\n",
    ")\n",
    "\n",
    "# Initialisations.\n",
    "matplotlib.rc(\"figure\", figsize=(15, 5))\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd83c28",
   "metadata": {},
   "source": [
    "# Implementing an SVC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practical assignment consists in implementing a simplified version of the SVC class of *scikit-learn*.\n",
    "\n",
    "This class will contain two fundamentals routines, one for training a non-linear SVC (using the RBF kernel), based on the SMO algorithm for solving the dual optimization problem, and the other one for predicting over new out-of-sample points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMO Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Problem\n",
    "\n",
    "The dual optimization problem for the Soft-Margin SVM is:\n",
    "$$\n",
    " \\left \\{ \\begin{array}{l} \\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^N} \\frac{1}{2} \\boldsymbol{\\alpha}^\\intercal \\tilde{\\mathbf{K}} \\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^\\intercal \\mathbf{1} \\\\ \\text{ s.t. } \\boldsymbol{\\alpha}^\\intercal \\mathbf{y} = 0 , 0 \\le \\boldsymbol{\\alpha} \\le C ,\n",
    " \\end{array} \\right .\n",
    "$$\n",
    "where $\\tilde{k}_{ij} = y_i y_j \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_j)$.\n",
    "\n",
    "This optimization problem is a QP problem, which is usually solved using an *ad hoc* algorithm: Sequential Minimal Optimization (SMO).\n",
    "\n",
    "### SMO and Coordinate Descent\n",
    "\n",
    "The main idea behind SMO is that the optimization problem is too involved to be solved as a whole, but it can be largely simplified considering only a few variables at a time.\n",
    "In this divide-and-conquer approach, called coordinate descent, the objective function is iteratively minimized over the different variables.\n",
    "\n",
    "In particular, SMO deals with two variables, the smaller working group that can be modified while respecting the constraints.\n",
    "Formally, the algorithm updates the current $\\alpha^\\text{old}_i$ and $\\alpha^\\text{old}_j$ coefficients to some new values $\\alpha_i$ and $\\alpha_j$, assuming that $\\boldsymbol{\\alpha}^\\text{old}$ satisfies $(\\boldsymbol{\\alpha}^\\text{old})^\\intercal \\mathbf{y} = 0$. In order to keep respecting the constraint in the new estimation, the following equation needs to be fulfilled:\n",
    "\\begin{align*}\n",
    " & y_i \\alpha^\\text{old}_i + y_j \\alpha^\\text{old}_j = y_i \\alpha_i + y_j \\alpha_j \\\\\n",
    " &\\implies y_i (\\alpha_i - \\alpha^\\text{old}_i) = - y_j (\\alpha_j - \\alpha^\\text{old}_j) \\\\\n",
    " &\\implies \\alpha_i = \\alpha^\\text{old}_i - y_i y_j (\\alpha_j - \\alpha^\\text{old}_j) .\n",
    "\\end{align*}\n",
    "\n",
    "For simplicity, one can assume that the update for the $j$-th coefficient is given by $\\alpha_j = \\alpha^\\text{old}_j + d$, and hence the optimization problem becomes a simple one-dimensional minimization.\n",
    "Therefore, it can be solved by computing the derivative with respect to $d$ and equating to $0$.\n",
    "After some derivations, the optimum unconstrained update turns out to be:\n",
    "\\begin{align*}\n",
    " d &= \\frac{y_j (E(\\mathbf{x}_j, y_j) - E(\\mathbf{x}_i, y_i))}{\\kappa} , \\text{ where} \\\\\n",
    " \\kappa &= 2 \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_j) - \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_i) - \\mathcal{K}(\\mathbf{x}_j, \\mathbf{x}_j) , \\text{ and} \\\\\n",
    " E(\\mathbf{x}, y) &= \\sum_{i = 1}^N \\alpha^\\text{old}_i y_i \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}) - y .\n",
    "\\end{align*}\n",
    "\n",
    "### Clipping\n",
    "\n",
    "The previous update does not take into account the box constrains $0 \\le \\alpha_i, \\alpha_j \\le C$.\n",
    "In order to satisfy these constraints for both $\\alpha_i$ and $\\alpha_j$, $\\alpha_j$ has to be clipped to the interval $[L, H]$, where\n",
    "$$\n",
    " \\begin{cases}\n",
    "  L = \\max(0, \\alpha^\\text{old}_j - \\alpha^\\text{old}_i), & H = \\min(C, C - \\alpha^\\text{old}_i + \\alpha^\\text{old}_j) & \\text{if } y_i \\neq y_j , \\\\\n",
    "  L = \\max(0, \\alpha^\\text{old}_j + \\alpha^\\text{old}_i - C), & H = \\min(C, \\alpha^\\text{old}_i + \\alpha^\\text{old}_j) & \\text{if } y_i = y_j .\n",
    " \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, the update for $\\alpha_j$ becomes:\n",
    "$$ \\alpha_j = \\min(\\max(\\alpha^\\text{old}_j + d, L), H) , $$\n",
    "and, as mentioned above, the corresponding update for $\\alpha_i$ is:\n",
    "$$ \\alpha_i = \\alpha^\\text{old}_i - y_i y_j (\\alpha_j - \\alpha^\\text{old}_j) . $$\n",
    "\n",
    "### Selecting the Working Pair\n",
    "\n",
    "There are different methods for selecting the working pair of coefficients $(i, j)$ that will be updated.\n",
    "Usually, first or second order information is used to choose the working set, trying to find those that will provide the largest minimization of the objective function.\n",
    "\n",
    "Nevertheless, in this work a simplified version of SMO will be implemented.\n",
    "The first coefficient $i$ will be selected just iteratively, cycling over all the possible values $i = 1, 2, \\dotsc, N$. The second coefficient $j$ is selected randomly over the remaining possible values $j \\neq i$.\n",
    "\n",
    "It should be noted that this approach will not guarantee the convergence in general, since not all possible pairs $(i, j)$ are explored, but it will work in most problems.\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "The algorithm will be terminated when a certain maximum number of iterations is reached (for each iteration, all the coefficients are updated), or when after one iteration the total change of the coefficients is smaller than a certain tolerance, $\\| \\boldsymbol{\\alpha} - \\boldsymbol{\\alpha}^\\text{old}\\| < \\epsilon$.\n",
    "\n",
    "Since the working pair is generated randomly, it can happen that after a whole iteration the changes are smaller than $\\epsilon$ not because the optimization is finished, but because of \"bad luck\". A workaround is to set $\\epsilon = 0$ to force all the iterations.\n",
    "\n",
    "### Computing the Intercept\n",
    "\n",
    "Once the optimum dual coefficients $\\boldsymbol{\\alpha}^*$ have been estimated, the intercept $b^*$ can be computed knowing that any support vector $\\mathbf{x}_k$ with $0 < \\alpha^*_k < C$ satisfies:\n",
    "$$ b^* = y_k - \\sum_{i = 1}^N \\alpha^*_i y_i \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_k) . $$\n",
    "\n",
    "This equality is usually averaged over all the possible vectors $k$ satisfying $0 < \\alpha^*_k < C$ for reasons of numerical stability.\n",
    "\n",
    "### Prediction\n",
    "\n",
    "Once the model is trained and the optimal parameters $\\boldsymbol{\\alpha}^*$ and $b^*$ are known, the score predicted over a new sample $\\mathbf{x}$ is:\n",
    "$$ f(\\mathbf{x}) = \\sum_{i = 1}^N \\alpha^*_i y_i \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}) + b^* . $$\n",
    "\n",
    "Finally, the class is assigned with:\n",
    "$$ \\hat{y} = \\operatorname{sign}(f(\\mathbf{x})) . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the SVC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "The objective of this assignment is to complete the class `MySVC` sketched below, which should contain at least the following methods.\n",
    "\n",
    "---\n",
    "```python\n",
    "__init__(self, C=1.0, gamma=\"scale\", tol=0.0001, max_iter=10000)\n",
    "```\n",
    "* This is the construction method for the class, with the following parameters:\n",
    "    * `C`: Regularization parameter $C$ of the SVC.\n",
    "    * `gamma`: Kernel parameter $\\gamma$ of the RBF kernel.\n",
    "    * `tol`: Tolerance for the stopping criterion $\\epsilon$.\n",
    "    * `max_iter`: Maximum number of iterations (in the sense of epochs; each iteration contains a pass through all the coefficients).\n",
    "* This method should only store the parameters in fields of the class, to be used when needed.\n",
    "\n",
    "---\n",
    "```python\n",
    "fit(self, x, y)\n",
    "```\n",
    "* This is the training method, with the following parameters:\n",
    "    * `x`: Training data matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$.\n",
    "    * `y`: Vector of labels $\\mathbf{y}$, with $y_i \\in \\{-1, +1\\}$.\n",
    "* This method should solve the dual optimization problem, storing the obtained coefficients and other useful information in fields of the class.\n",
    "\n",
    "---\n",
    "```python\n",
    "decision_function(self, x)\n",
    "```\n",
    "* This is the method that computes the output of the SVC, with the following parameters:\n",
    "    * `x`: Test data matrix $\\mathbf{X}' \\in \\mathbb{R}^{N' \\times d}$.\n",
    "* This method should predict the output (the score) of the SVC, using the information stored in the fields of the class, and return it as a vector.\n",
    "\n",
    "---\n",
    "```python\n",
    "predict(self, x)\n",
    "```\n",
    "* This is the method that computes the predicted class of the SVC, with the following parameters:\n",
    "    * `x`: Test data matrix $\\mathbf{X}' \\in \\mathbb{R}^{N' \\times d}$.\n",
    "* This method should predict the assigned class of the SVC, using the information stored in the fields of the class, and return it as a vector.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "Additional auxiliary methods can be defined to simplify the code.\n",
    "Some suggestions are:\n",
    "\n",
    "1. A method to compute the output of the SVC, with the current estimated parameters but without considering the intercept:\n",
    "$$ \\sum_{i = 1}^N \\alpha_i y_i \\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}) . $$\n",
    "1. A method to compute $E(\\mathbf{x}, y)$.\n",
    "1. A method to compute the lower and upper bounds $L$ and $H$ for the clipping of the update.\n",
    "1. A method to estimate $b^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Complete the `MySVC` class below, satisfying the described requirements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Insert code. Additional imports.\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class MySVC:\n",
    "    \"\"\"\n",
    "    SVC with a simplified version of SMO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, C=1.0, gamma=\"scale\", tol=0.0001, max_iter=100):\n",
    "        ########################################################################\n",
    "        # Insert code. Assignment of the hyper-parameters (complete).\n",
    "\n",
    "        ########################################################################\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Constants.\n",
    "        n_pat = x.shape[0]\n",
    "        n_dim = x.shape[1]\n",
    "\n",
    "        # Options for gamma (for compatibility with sklean).\n",
    "        if self.gamma == \"scale\":\n",
    "            self.gamma = 1.0 / (n_dim * x.var())\n",
    "        if self.gamma == \"auto\":\n",
    "            self.gamma = 1.0 / n_dim\n",
    "\n",
    "        # Initialization of the dual coefficients (named \"a\" instead of \"alpha\" for simplicity).\n",
    "        self.a = np.zeros(n_pat)\n",
    "\n",
    "        ########################################################################\n",
    "        # Insert code. Other initializations (complete).\n",
    "\n",
    "        ########################################################################\n",
    "\n",
    "        # Loop over the iterations.\n",
    "        for it in range(self.max_iter):\n",
    "            ####################################################################\n",
    "            # Insert code. Initializations (complete).\n",
    "\n",
    "            ####################################################################\n",
    "\n",
    "            # Loop over the coefficients.\n",
    "            for i in range(0, n_pat):\n",
    "                j = self.choose_j(i)\n",
    "                ################################################################\n",
    "                # Insert code. Update of the corresponding a[i] and a[j] values (complete).\n",
    "\n",
    "                ################################################################\n",
    "\n",
    "            ####################################################################\n",
    "            # Insert code. Check of the stopping conditions (complete).\n",
    "\n",
    "            ####################################################################\n",
    "\n",
    "        ########################################################################\n",
    "        # Insert code. Storage of the obtained parameters and computation of the intercept (complete).\n",
    "\n",
    "        ########################################################################\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, x):\n",
    "        ########################################################################\n",
    "        # Insert code. Computation of the decision function over x (complete).\n",
    "\n",
    "        ########################################################################\n",
    "\n",
    "    def predict(self, x):\n",
    "        ########################################################################\n",
    "        # Insert code. Computation of the predicted class over x (complete).\n",
    "\n",
    "        ########################################################################\n",
    "\n",
    "    def choose_j(self, i):\n",
    "        v = np.arange(len(self.a))\n",
    "        return np.random.choice(v[v != i])\n",
    "\n",
    "    ############################################################################\n",
    "    # Insert code. Auxiliary methods (complete if needed).\n",
    "\n",
    "    ############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative with *scikit-learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell generates the *moons* dataset, dividing it into train and test splits, and depicting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_dataset_moons(seed)\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(x, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "plot_dataset_clas(x_tr, y_tr)\n",
    "plt.title(\"Train\")\n",
    "plt.show()\n",
    "plot_dataset_clas(x_te, y_te)\n",
    "plt.title(\"Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Train an SVC model over the training data (`x_tr`, `y_tr`), using both the `SVC` class of *scikit-learn* and the implemented `MySVC` class.\n",
    "* Predict using the previous two models over the test data (`x_te`, `y_te`).\n",
    "* Compare that both classes provide the same outputs (in terms of the decision function and of the predicted class), describing the obtained results.\n",
    "  If needed, reduce the default stopping tolerance and/or increase the default maximum number of iterations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "gamma = \"scale\"\n",
    "\n",
    "model_my = MySVC(C=C, gamma=gamma, tol=0)\n",
    "model_sk = SVC(C=C, gamma=gamma)\n",
    "\n",
    "################################################################################\n",
    "# Insert code.\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"qst\">\n",
    "\n",
    "* Considering the model created with the `MySVC` class:\n",
    "    * Extract the list of support vectors, explaining the followed approach.\n",
    "    * Extract the list of training points bad and well classified, explaining the followed approach.\n",
    "    * Extract the list of training points that lie over the supporting hyperplanes.\n",
    "    * Extract the list of training points located on the wrong side of the supporting hyperplanes, but that are well classified.\n",
    "* What are the conditions over the dual coefficients $\\alpha_i$ that characterize the previous lists (if any)?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Insert code.\n",
    "\n",
    "################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
